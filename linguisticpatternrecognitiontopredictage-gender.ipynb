{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3381,"sourceType":"datasetVersion","datasetId":1968}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T08:31:18.135997Z","iopub.execute_input":"2024-04-05T08:31:18.136681Z","iopub.status.idle":"2024-04-05T08:31:19.636485Z","shell.execute_reply.started":"2024-04-05T08:31:18.136598Z","shell.execute_reply":"2024-04-05T08:31:19.635180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/blog-authorship-corpus/blogtext.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:31:19.638805Z","iopub.execute_input":"2024-04-05T08:31:19.639536Z","iopub.status.idle":"2024-04-05T08:31:41.411467Z","shell.execute_reply.started":"2024-04-05T08:31:19.639487Z","shell.execute_reply":"2024-04-05T08:31:41.410025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  df.drop(['id', 'topic', 'sign', 'date'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:31:41.413903Z","iopub.execute_input":"2024-04-05T08:31:41.414312Z","iopub.status.idle":"2024-04-05T08:31:41.478152Z","shell.execute_reply.started":"2024-04-05T08:31:41.414275Z","shell.execute_reply":"2024-04-05T08:31:41.476890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:31:41.479788Z","iopub.execute_input":"2024-04-05T08:31:41.480133Z","iopub.status.idle":"2024-04-05T08:31:41.495479Z","shell.execute_reply.started":"2024-04-05T08:31:41.480104Z","shell.execute_reply":"2024-04-05T08:31:41.494095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working//blogtext.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:31:41.498705Z","iopub.execute_input":"2024-04-05T08:31:41.499058Z","iopub.status.idle":"2024-04-05T08:32:20.470908Z","shell.execute_reply.started":"2024-04-05T08:31:41.499026Z","shell.execute_reply":"2024-04-05T08:32:20.469819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom multiprocessing import Pool, cpu_count\n\n# Download NLTK stop words data\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet', '/usr/share/nltk_data')\n\nfrom nltk.corpus import wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:32:20.472363Z","iopub.execute_input":"2024-04-05T08:32:20.473048Z","iopub.status.idle":"2024-04-05T08:32:24.796284Z","shell.execute_reply.started":"2024-04-05T08:32:20.473009Z","shell.execute_reply":"2024-04-05T08:32:24.795079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_char(text):\n    pattern = r'[^a-zA-Z0-9\\s]'\n    cleaned_text = re.sub(pattern, '', text)\n    return cleaned_text\ndef preprocessing(text):\n    \n    text = remove_special_char(text)\n    words = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n    filtered_text = ' '.join(filtered_words)\n    return filtered_text\n\ndef process_chunk(chunk):\n    chunk['text'] = chunk['text'].apply(preprocessing)\n    return chunk\n\ndef apply_preprocessing(File):\n    chunks = pd.read_csv(File, chunksize=1000)\n    \n    pool = Pool(cpu_count())\n    \n    processed_chunks = pool.map(process_chunk, chunks)\n    \n    pool.close()\n    pool.join()\n    \n    processed_df = pd.concat(processed_chunks)\n    \n    processed_df.to_csv(File, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:32:24.798223Z","iopub.execute_input":"2024-04-05T08:32:24.798685Z","iopub.status.idle":"2024-04-05T08:32:24.811712Z","shell.execute_reply.started":"2024-04-05T08:32:24.798645Z","shell.execute_reply":"2024-04-05T08:32:24.810172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_preprocessing('/kaggle/working//blogtext.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:32:24.814059Z","iopub.execute_input":"2024-04-05T08:32:24.814640Z","iopub.status.idle":"2024-04-05T08:44:42.446857Z","shell.execute_reply.started":"2024-04-05T08:32:24.814596Z","shell.execute_reply":"2024-04-05T08:44:42.445279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working//blogtext.csv')\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:02:34.839106Z","iopub.execute_input":"2024-04-05T06:02:34.839645Z","iopub.status.idle":"2024-04-05T06:02:42.251237Z","shell.execute_reply.started":"2024-04-05T06:02:34.839613Z","shell.execute_reply":"2024-04-05T06:02:42.249394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_values = df.isna().sum()\n\nprint(\"The number of NaN in each column is :\")\nprint(nan_values)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:45:44.988030Z","iopub.execute_input":"2024-04-05T08:45:44.989045Z","iopub.status.idle":"2024-04-05T08:45:45.494670Z","shell.execute_reply.started":"2024-04-05T08:45:44.988996Z","shell.execute_reply":"2024-04-05T08:45:45.493257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:50:56.927436Z","iopub.execute_input":"2024-04-05T08:50:56.927884Z","iopub.status.idle":"2024-04-05T08:51:05.767534Z","shell.execute_reply.started":"2024-04-05T08:50:56.927853Z","shell.execute_reply":"2024-04-05T08:51:05.766219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef preprocess_for_bert(text):\n    if isinstance(text, str):\n        tokens = tokenizer.tokenize(text)\n    \n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n    \n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    \n        max_length = 128\n        input_ids = input_ids[:max_length] + [0] * (max_length - len(input_ids))\n    else:\n        return None\n    return input_ids\n\ndef process_chunk_for_bert(chunk):\n    chunk['input_ids'] = chunk['text'].apply(preprocess_for_bert)\n    return chunk\n\ndef apply_preprocess_for_bert(File):\n    chunks = pd.read_csv(File, chunksize=1000)\n    \n    pool = Pool(cpu_count())\n    \n    processed_chunks = pool.map(process_chunk_for_bert, chunks)\n    \n    pool.close()\n    pool.join()\n    \n    processed_df = pd.concat(processed_chunks)\n    \n    processed_df.to_csv(File, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:51:09.547088Z","iopub.execute_input":"2024-04-05T08:51:09.547693Z","iopub.status.idle":"2024-04-05T08:51:09.557156Z","shell.execute_reply.started":"2024-04-05T08:51:09.547659Z","shell.execute_reply":"2024-04-05T08:51:09.555817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_preprocess_for_bert('/kaggle/working/blogtext.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:51:20.124203Z","iopub.execute_input":"2024-04-05T08:51:20.124633Z","iopub.status.idle":"2024-04-05T09:14:49.742996Z","shell.execute_reply.started":"2024-04-05T08:51:20.124602Z","shell.execute_reply":"2024-04-05T09:14:49.741033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/blogtext.csv')\n\nnan_values = df.isna().sum()\n\nprint(\"The number of NaN in each column is :\")\nprint(nan_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:15:59.896231Z","iopub.execute_input":"2024-04-05T09:15:59.896866Z","iopub.status.idle":"2024-04-05T09:16:11.619953Z","shell.execute_reply.started":"2024-04-05T09:15:59.896823Z","shell.execute_reply":"2024-04-05T09:16:11.618529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:17:55.936124Z","iopub.execute_input":"2024-04-05T09:17:55.936916Z","iopub.status.idle":"2024-04-05T09:17:55.943256Z","shell.execute_reply.started":"2024-04-05T09:17:55.936884Z","shell.execute_reply":"2024-04-05T09:17:55.942236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['text'], axis=1)\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:20:08.037350Z","iopub.execute_input":"2024-04-05T09:20:08.037856Z","iopub.status.idle":"2024-04-05T09:20:08.474386Z","shell.execute_reply.started":"2024-04-05T09:20:08.037822Z","shell.execute_reply":"2024-04-05T09:20:08.473035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_values = df.isna().sum()\n\nprint(\"The number of NaN in each column is :\")\nprint(nan_values)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:20:23.963534Z","iopub.execute_input":"2024-04-05T09:20:23.963989Z","iopub.status.idle":"2024-04-05T09:20:24.159019Z","shell.execute_reply.started":"2024-04-05T09:20:23.963954Z","shell.execute_reply":"2024-04-05T09:20:24.157268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/blogtext.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:21:38.095592Z","iopub.execute_input":"2024-04-05T09:21:38.096414Z","iopub.status.idle":"2024-04-05T09:21:57.308023Z","shell.execute_reply.started":"2024-04-05T09:21:38.096366Z","shell.execute_reply":"2024-04-05T09:21:57.305856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-04-05T09:24:49.131569Z","iopub.execute_input":"2024-04-05T09:24:49.132065Z","iopub.status.idle":"2024-04-05T09:24:50.739901Z","shell.execute_reply.started":"2024-04-05T09:24:49.132032Z","shell.execute_reply":"2024-04-05T09:24:50.738944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertModel.from_pretrained('bert_base_uncased', output_hidden_states=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bert_embeddings(input_ids):\n    \n    input_ids_tensor = torch.tensor(input_ids)\n    \n    model.eval()\n    \n    with torch.no_grad:\n        outputs = model(input_ids_tensor)\n    \n    hidden_stated = outputs.hidden_states\n    \n    last_layer_embeddings = hidden_states[-1]\n    \n    cls_embeddings = torch.mean(last_layer_embeddings, dim=1)\n    \n    return cls_embeddings\n\ndef get_chunk_embeddings(chunks):\n    chunk['bert_embeddings'] = chunk['input_ids'].apply(preprocess_for_bert)\n    return chunk\n\ndef apply_embedding_extraction(File):\n    chunks = pd.read_csv(File, chunksize=1000)\n    \n    pool = Pool(cpu_count())\n    \n    processed_chunks = pool.map(get_chunk_embeddings, chunks)\n    \n    pool.close()\n    pool.join()\n    \n    processed_df = pd.concat(processed_chunks)\n    \n    processed_df.to_csv(File, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:18:29.548586Z","iopub.execute_input":"2024-04-05T16:18:29.549014Z","iopub.status.idle":"2024-04-05T16:18:29.556554Z","shell.execute_reply.started":"2024-04-05T16:18:29.548982Z","shell.execute_reply":"2024-04-05T16:18:29.555324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_embedding_extraction('/kaggle/working/blogtext.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}